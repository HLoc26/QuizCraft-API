# ğŸ“˜ QuizCraft â€“ AI-Powered Self Learning

This project allows users to **upload documents (PDF, DOCX, images, scans)** â†’ the system will:  
1. **OCR / Extract text** (if the file is an image or scanned PDF).  
2. **Split text into chunks**.  
3. **Generate multiple-choice questions (MCQs)** from the content using **Ollama + LLM (Llama3.2)**.  
4. Return clean JSON so that quizzes can be displayed in a frontend.  

---

## ğŸš€ Tech Stack

- **Backend**: [FastAPI](https://fastapi.tiangolo.com/)  
- **AI/LLM**: [Ollama](https://ollama.ai/) (running in Docker, model `llama3.2`)  
- **OCR**: [PyMuPDF](https://pymupdf.readthedocs.io/), [python-docx](https://python-docx.readthedocs.io/), [Pillow](https://python-pillow.org/)  
- **Async**: Python `asyncio`  
- **Architecture**: Clean separation of `controller` / `service` / `schemas`  

---

## ğŸ“‚ Project Structure

```

src/
â”‚â”€â”€ main.py              # FastAPI entry point
â”‚â”€â”€ constants/           # constants (OLLAMA_URL, prompt templatesâ€¦)
â”‚â”€â”€ schemas/             # Pydantic request/response models
â”‚â”€â”€ controllers/         # FastAPI routers
â”‚â”€â”€ services/            # business logic (OCR, MCQ, Uploadâ€¦)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scan_service.py
â”‚   â”œâ”€â”€ upload_service.py
â”‚   â”œâ”€â”€ ocr_service.py
â”‚   â””â”€â”€ mcq_service.py
â”‚â”€â”€ utils/               # helper utilities (TextHelperâ€¦)

````

---

## âš™ï¸ Setup

### 1. Clone repo
```bash
git clone https://github.com/hloc26/QuizCraft-API.git
cd QuizCraft-API
````

### 2. Create virtual environment & install dependencies

```bash
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
.venv\Scripts\activate     # Windows

pip install -r requirements.txt
```

If you want to use your GPU, make sure you have CUDA installed, and uncomment the last line in requirements.txt 

```
# -f https://download.pytorch.org/whl/cu129 <- Uncomment this
```

### 3. Run Ollama in Docker

```bash
docker run -d --name ollama -p 11434:11434 ollama/ollama
docker exec -it ollama ollama pull llama3.2
```

### 4. Run FastAPI server

```bash
fastapi dev src/main.py
```

Open [http://localhost:8000/docs](http://localhost:8000/docs) to test the API.

---

## ğŸ“¡ API Endpoints

### Upload + OCR

```http
POST /upload
```

* Accepts PDF/DOCX/IMG/TXT files, stores them on the server, and extracts text.

### Generate MCQs

```http
POST /mcq/generate
```

Request body:

```json
{
  "text": "Some content to generate questions from",
  "max_chunk_word": 200,
  "question_per_chunk": 3
}
```

Response:

```json
{
  "questions": [
    {
      "question": "When was George Washington born?",
      "options": ["February 22", "December 22", "February 12", "February 15"],
      "answer": "February 22",
      "source_chunk_index": 0
    }
  ]
}
```

---

## ğŸ“ Roadmap

* [ ] Add language selection (English / Vietnamese).
* [ ] Improve distractor (wrong answers) generation.
* [ ] Add frontend UI for quizzes.
* [ ] Support more LLMs (Mistral, Gemma, etc).

---

## ğŸ“œ License

MIT License. Free for personal & educational use.
